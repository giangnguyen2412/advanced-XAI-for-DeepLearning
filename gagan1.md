# Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance

## Notes 

- We show that explanations increase the chance
that humans will accept the AI’s recommendation regardless of whether the AI is correct. 
- when the AI provides explanations, team accuracy reaches a level higher than human-alone
- To explore these questions, we conduct new studies where we control the study design such
that the AI’s accuracy is comparable to the human’s (Figure 1B). This decision aims at simulating
a setting where (i) there is a stronger incentive to deploy hybrid human-AI teams, and (ii) there
exists more potential for complementary performance
- we varied the representation of explanations by
explaining just the predicted class versus explaining the top-two classes
- We observed complementary performance on every task, but surprisingly,
explanations did not yield improved team performance, compared to simply showing the AI’s
confdence. Explanations increased team performance when the AI system was correct, but
decreased it when the AI erred — so the net value was minimal

## Limitations

- We focused solely on the use of explanations to improve the accuracy of human-AI teams. As
mentioned before, AI explanations have other motivations not addressed by this paper, such as
debugging AI output [56], teaching [48], satisfying legal requirements [64], and increasing trust
between sub-organizations [27]. 

## Conclusion

To our surprise, the simple approach of showing the AI’s confdence worked as well as any
of the actual explanations strategies that we considered. 

The use of explanations increased the
human’s blind trust in the AI system, rather than appropriate reliance
