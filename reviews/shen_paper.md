# How Useful Are the Machine-Generated Interpretations to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels

## Summary

This paper presents an investigation on whether or not
showing machine-generated visual interpretations helps users
understand the incorrectly predicted labels produced by image classifiers. We showed the images and the correct labels
to 150 online crowd workers and asked them to select the incorrectly predicted labels with or without showing them the
machine-generated visual interpretations. The results demonstrated that displaying the visual interpretations did not increase, but rather decreased, the average guessing accuracy
by roughly 10%.

## My thoughts
The paper is focusing on how interpretation helps to identify incorrectly predicted images by using general users (Need to explain to them what is the saliency maps by words). Then required them to identified the wrongly predicted label from 5 labels. 

Exp1 is unfair as two sets are differrent and the numbers of images for each case (C1-5) are unbalanced. I think the setup is poor in this paper.

This paper aims to answer a very detailed question. When AI is wrong, is interpretation useful or harmful to identify the predicted label. IMO, I think this question is meaningless in practice. Why we want to know what is the predicted label when we know AI is wrong? 

Explaining why AI is wrong is useful, but it is not the research question of this paper. 
