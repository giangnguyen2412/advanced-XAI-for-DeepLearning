# Summary
Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models.
Authors present a general method for visualizing and understanding GANs at different levels of abstraction, from each neuron, to each object, to the contextual relationship between different objects. They first identify a group of interpretable units that are related to object concepts. Second, they directly intervene within the network to identify sets of units that cause a type of objects to disappear or appear (for example buildings have door but trees do not), since quantifying the causal effect of these units using a standard causality metric. Finally, authors examine the contextual relationship between these units and their surrounding by inserting the discovered object concepts into new images. Researchers study where they can insert object concepts in new images and how this intervention interacts with other objects in the image.
Also, they show several practical applications enabled by the framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene.
# Contribution
This paper gives us huge understanding of GANs as well as compare, debug, modify, and reason about a GAN models. There is no theoretical contribution or any deep analysis since the paper presents a new methodological idea, which allows for nice practical contribution but it can help further researches to be significantly explicit and powerful.
# Comment
The paper is well-written and organized. The dissection and intervention for finding relationships between representation units and objects are simple, straightforward and meaningful. Almost representation focus on the generator, it provides a window into the internal mechanisms of a GAN.
# Conclusion
This is one of the first extensive studies that target the understanding and visualization of generative models. Focusing on the most popular generative model – Generative Adversarial Networks, it reveals significant insights about generative models. One of the main findings is that the larger part of GAN representations can be interpreted. It shows that GAN’s internal representation encodes variables that have a causal effect on the generation of objects and realistic images. This work provides a basis for analysis, debugging and understanding of Generative Adversarial Network models.
